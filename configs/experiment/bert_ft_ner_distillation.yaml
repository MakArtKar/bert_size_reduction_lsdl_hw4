# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - bert_ft_ner_factorized_embeddings

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["ner", "bert-base-cased", "factorized_embeddings", "distillation", "prajjwal1/bert-mini"]

model:
  _target_: src.models.ner_distillation_module.NERDistillationLitModule
  model_name:
    _target_: transformers.BertForTokenClassification
    config:
      _target_: transformers.BertConfig
      vocab_size: 28996
      hidden_size: 512
      num_attention_heads: 8
      num_hidden_layers: 8
      intermediate_size: 1024
      num_labels: 9
  teacher_model_name: bert-base-cased
  factorized_embeddings_hidden_size: 64
  optimizer:
    lr: 5e-4

data:
  batch_size: 256

preprocess_ckpt_path:
  net: teacher

logger:
  wandb:
    tags: ${tags}
    name: "bert_ft_ner_distillation"
    project: "bert_size_reduction_lsdl_hw4"
  aim:
    experiment: "bert_ft_ner_distillation"

